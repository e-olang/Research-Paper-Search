{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load sw portion of the dataset\n",
    "data = load_dataset('xnli', 'sw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 5010\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 2490\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'Sakata cream ya conceptually ina vipimo viwili vya msingi - bidhaa na geography .',\n",
       " 'hypothesis': 'Bidhaa na geography ndio hufanya kazi ya utoaji wa cream .',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0 — entailment, e.g. the premise suggests the hypothesis.\n",
    "- 1 — neutral, the premise and hypothesis could both be true, but they are not necessarily related.\n",
    "- 2 — contradiction, the premise and hypothesis contradict each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['entailment', 'neutral', 'contradiction']\n"
     ]
    }
   ],
   "source": [
    "# print all labels\n",
    "print(data['train'].features['label'].names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dic = {'neutral': 1, 'contradiction': 2, 'entailment': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017639875411987305,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 393,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3a62771ca14363b977d58344b7ba98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/393 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/olang/Desktop/*/Projects/Sentence-Transformers/train sw.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/olang/Desktop/%2A/Projects/Sentence-Transformers/train%20sw.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m all_cols \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/olang/Desktop/%2A/Projects/Sentence-Transformers/train%20sw.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m part \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mpremise\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhypothesis\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/olang/Desktop/%2A/Projects/Sentence-Transformers/train%20sw.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/olang/Desktop/%2A/Projects/Sentence-Transformers/train%20sw.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39mlambda\u001b[39;49;00m x: tokenizer(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/olang/Desktop/%2A/Projects/Sentence-Transformers/train%20sw.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m             x[part], max_length\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m, padding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/olang/Desktop/%2A/Projects/Sentence-Transformers/train%20sw.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m             truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/olang/Desktop/%2A/Projects/Sentence-Transformers/train%20sw.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         ), batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/olang/Desktop/%2A/Projects/Sentence-Transformers/train%20sw.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/olang/Desktop/%2A/Projects/Sentence-Transformers/train%20sw.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/olang/Desktop/%2A/Projects/Sentence-Transformers/train%20sw.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mrename_column(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/olang/Desktop/%2A/Projects/Sentence-Transformers/train%20sw.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m             col, part\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mcol\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/olang/Desktop/%2A/Projects/Sentence-Transformers/train%20sw.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/nlp/lib/python3.9/site-packages/datasets/dataset_dict.py:816\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    814\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[1;32m    815\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 816\u001b[0m     {\n\u001b[1;32m    817\u001b[0m         k: dataset\u001b[39m.\u001b[39mmap(\n\u001b[1;32m    818\u001b[0m             function\u001b[39m=\u001b[39mfunction,\n\u001b[1;32m    819\u001b[0m             with_indices\u001b[39m=\u001b[39mwith_indices,\n\u001b[1;32m    820\u001b[0m             with_rank\u001b[39m=\u001b[39mwith_rank,\n\u001b[1;32m    821\u001b[0m             input_columns\u001b[39m=\u001b[39minput_columns,\n\u001b[1;32m    822\u001b[0m             batched\u001b[39m=\u001b[39mbatched,\n\u001b[1;32m    823\u001b[0m             batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m    824\u001b[0m             drop_last_batch\u001b[39m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    825\u001b[0m             remove_columns\u001b[39m=\u001b[39mremove_columns,\n\u001b[1;32m    826\u001b[0m             keep_in_memory\u001b[39m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    827\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    828\u001b[0m             cache_file_name\u001b[39m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    829\u001b[0m             writer_batch_size\u001b[39m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    830\u001b[0m             features\u001b[39m=\u001b[39mfeatures,\n\u001b[1;32m    831\u001b[0m             disable_nullable\u001b[39m=\u001b[39mdisable_nullable,\n\u001b[1;32m    832\u001b[0m             fn_kwargs\u001b[39m=\u001b[39mfn_kwargs,\n\u001b[1;32m    833\u001b[0m             num_proc\u001b[39m=\u001b[39mnum_proc,\n\u001b[1;32m    834\u001b[0m             desc\u001b[39m=\u001b[39mdesc,\n\u001b[1;32m    835\u001b[0m         )\n\u001b[1;32m    836\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    837\u001b[0m     }\n\u001b[1;32m    838\u001b[0m )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/nlp/lib/python3.9/site-packages/datasets/dataset_dict.py:817\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    814\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[1;32m    815\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    816\u001b[0m     {\n\u001b[0;32m--> 817\u001b[0m         k: dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m    818\u001b[0m             function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m    819\u001b[0m             with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m    820\u001b[0m             with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m    821\u001b[0m             input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m    822\u001b[0m             batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m    823\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    824\u001b[0m             drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m    825\u001b[0m             remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m    826\u001b[0m             keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m    827\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m    828\u001b[0m             cache_file_name\u001b[39m=\u001b[39;49mcache_file_names[k],\n\u001b[1;32m    829\u001b[0m             writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m    830\u001b[0m             features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m    831\u001b[0m             disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m    832\u001b[0m             fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m    833\u001b[0m             num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m    834\u001b[0m             desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m    835\u001b[0m         )\n\u001b[1;32m    836\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    837\u001b[0m     }\n\u001b[1;32m    838\u001b[0m )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/nlp/lib/python3.9/site-packages/datasets/arrow_dataset.py:2815\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   2812\u001b[0m disable_tqdm \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[1;32m   2814\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m num_proc \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 2815\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_single(\n\u001b[1;32m   2816\u001b[0m         function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m   2817\u001b[0m         with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m   2818\u001b[0m         with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m   2819\u001b[0m         input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m   2820\u001b[0m         batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m   2821\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   2822\u001b[0m         drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m   2823\u001b[0m         remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m   2824\u001b[0m         keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m   2825\u001b[0m         load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m   2826\u001b[0m         cache_file_name\u001b[39m=\u001b[39;49mcache_file_name,\n\u001b[1;32m   2827\u001b[0m         writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m   2828\u001b[0m         features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   2829\u001b[0m         disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m   2830\u001b[0m         fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m   2831\u001b[0m         new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint,\n\u001b[1;32m   2832\u001b[0m         disable_tqdm\u001b[39m=\u001b[39;49mdisable_tqdm,\n\u001b[1;32m   2833\u001b[0m         desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m   2834\u001b[0m     )\n\u001b[1;32m   2835\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2837\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mformat_cache_file_name\u001b[39m(cache_file_name, rank):\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/nlp/lib/python3.9/site-packages/datasets/arrow_dataset.py:546\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    545\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 546\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    547\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    548\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    549\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/nlp/lib/python3.9/site-packages/datasets/arrow_dataset.py:513\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    507\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    508\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    509\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    510\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    511\u001b[0m }\n\u001b[1;32m    512\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 513\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    514\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    515\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/nlp/lib/python3.9/site-packages/datasets/fingerprint.py:480\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    478\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 480\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    482\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/nlp/lib/python3.9/site-packages/datasets/arrow_dataset.py:3253\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   3251\u001b[0m                 writer\u001b[39m.\u001b[39mwrite_table(batch)\n\u001b[1;32m   3252\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3253\u001b[0m                 writer\u001b[39m.\u001b[39;49mwrite_batch(batch)\n\u001b[1;32m   3254\u001b[0m \u001b[39mif\u001b[39;00m update_data \u001b[39mand\u001b[39;00m writer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3255\u001b[0m     writer\u001b[39m.\u001b[39mfinalize()  \u001b[39m# close_stream=bool(buf_writer is None))  # We only close if we are writing in a file\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/nlp/lib/python3.9/site-packages/datasets/arrow_writer.py:551\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[0;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[1;32m    549\u001b[0m         col_try_type \u001b[39m=\u001b[39m try_features[col] \u001b[39mif\u001b[39;00m try_features \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m col \u001b[39min\u001b[39;00m try_features \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    550\u001b[0m         typed_sequence \u001b[39m=\u001b[39m OptimizedTypedSequence(col_values, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mcol_type, try_type\u001b[39m=\u001b[39mcol_try_type, col\u001b[39m=\u001b[39mcol)\n\u001b[0;32m--> 551\u001b[0m         arrays\u001b[39m.\u001b[39mappend(pa\u001b[39m.\u001b[39;49marray(typed_sequence))\n\u001b[1;32m    552\u001b[0m         inferred_features[col] \u001b[39m=\u001b[39m typed_sequence\u001b[39m.\u001b[39mget_inferred_type()\n\u001b[1;32m    553\u001b[0m schema \u001b[39m=\u001b[39m inferred_features\u001b[39m.\u001b[39marrow_schema \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpa_writer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/nlp/lib/python3.9/site-packages/pyarrow/array.pxi:229\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/nlp/lib/python3.9/site-packages/pyarrow/array.pxi:110\u001b[0m, in \u001b[0;36mpyarrow.lib._handle_arrow_array_protocol\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/nlp/lib/python3.9/site-packages/datasets/arrow_writer.py:189\u001b[0m, in \u001b[0;36mTypedSequence.__arrow_array__\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m     trying_cast_to_python_objects \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m     out \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39;49marray(cast_to_python_objects(data, only_1d_for_numpy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n\u001b[1;32m    190\u001b[0m \u001b[39m# use smaller integer precisions if possible\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrying_int_optimization:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_cols = ['label']\n",
    "\n",
    "for part in ['premise', 'hypothesis']:\n",
    "    dataset = dataset.map(\n",
    "        lambda x: tokenizer(\n",
    "            x[part], max_length=128, padding='max_length',\n",
    "            truncation=True\n",
    "        ), batched=True\n",
    "    )\n",
    "    for col in ['input_ids', 'attention_mask']:\n",
    "        dataset = dataset.rename_column(\n",
    "            col, part+'_'+col\n",
    "        )\n",
    "        all_cols.append(part+'_'+col)\n",
    "\n",
    "print(all_cols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4524e0aa81b3ae16e25dcf9b33e91112c133dfc9ef43cc517d5da5b8fe0f3eee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
