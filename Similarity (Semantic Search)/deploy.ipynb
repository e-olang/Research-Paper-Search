{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Files/titles.pkl', 'rb') as f:\n",
    "    titles = pickle.load(f)\n",
    "\n",
    "with open('Files/embeddings.pkl', 'rb') as f:\n",
    "    embeddings = pickle.load(f)\n",
    "\n",
    "with open('Files/authors.pkl', 'rb') as f:\n",
    "    authors = pickle.load(f)\n",
    "\n",
    "with open('Files/years.pkl', 'rb') as f:\n",
    "    years = pickle.load(f)\n",
    "\n",
    "with open('Files/summary.pkl', 'rb') as f:\n",
    "    summary = pickle.load(f)\n",
    "\n",
    "index = pickle.load(open('Files/index.sav' , 'rb'))\n",
    "\n",
    "#model = pickle.load(open('Files/model.sav' , 'rb'))\n",
    "model = SentenceTransformer('sentence-transformers/multi-qa-MiniLM-L6-cos-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple query to return the first 5 similar documents (Titles, Authors, Years, Summary)\n",
    "\n",
    "def retrieve(query, k=5):\n",
    "    xq = model.encode([query])\n",
    "    D, I = index.search(xq, k)\n",
    "\n",
    "    results = {'titles': [], 'authors': [], 'years': [], 'summary': []}\n",
    "    for i in range(k):\n",
    "        results['titles'].append(titles[I[0][i]])\n",
    "        results['authors'].append(authors[I[0][i]])\n",
    "        results['years'].append(years[I[0][i]])\n",
    "        results['summary'].append(summary[I[0][i]])\n",
    "    \n",
    "    return results\n",
    "    #return [titles[i] for i in I[0]], [authors[i] for i in I[0]], [years[i] for i in I[0]], [summary[i] for i in I[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = retrieve(str(input(\"Enter Query: \")))\n",
    "results = retrieve(str('Mathematical models of the spread of infectious diseases in humans and animals'))\n",
    "results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients' Severity States Classification based on Electronic Health\n",
      "  Record (EHR) Data using Multiple Machine Learning and Deep Learning\n",
      "  Approaches\n",
      "Annajiat Alim Rasel\n",
      "2022\n",
      "This research presents an examination of categorizing the severity states of\n",
      "patients based on their electronic health records during a certain time range\n",
      "using multiple machine learning and deep learning approaches. The suggested\n",
      "method uses an EHR dataset collected from an open-source platform to categorize\n",
      "severity. Some tools were used in this research, such as openRefine was used to\n",
      "pre-process, RapidMiner was used for implementing three algorithms (Fast Large\n",
      "Margin, Generalized Linear Model, Multi-layer Feed-forward Neural Network) and\n",
      "Tableau was used to visualize the data, for implementation of algorithms we\n",
      "used Google Colab. Here we implemented several supervised and unsupervised\n",
      "algorithms along with semi-supervised and deep learning algorithms. The\n",
      "experimental results reveal that hyperparameter-tuned Random Forest\n",
      "outperformed all the other supervised machine learning algorithms with 76%\n",
      "accuracy as well as Generalized Linear algorithm achieved the highest precision\n",
      "score 78%, whereas the hyperparameter-tuned Hierarchical Clustering with 86%\n",
      "precision score and Gaussian Mixture Model with 61% accuracy outperformed other\n",
      "unsupervised approaches. Dimensionality Reduction improved results a lot for\n",
      "most unsupervised techniques. For implementing Deep Learning we employed a\n",
      "feed-forward neural network (multi-layer) and the Fast Large Margin approach\n",
      "for semi-supervised learning. The Fast Large Margin performed really well with\n",
      "a recall score of 84% and an F1 score of 78%. Finally, the Multi-layer\n",
      "Feed-forward Neural Network performed admirably with 75% accuracy, 75%\n",
      "precision, 87% recall, 81% F1 score.\n",
      "--------------------------------------------------------------------------------\n",
      "What Do You See in this Patient? Behavioral Testing of Clinical NLP\n",
      "  Models\n",
      "Alexander Löser\n",
      "2021\n",
      "Decision support systems based on clinical notes have the potential to\n",
      "improve patient care by pointing doctors towards overseen risks. Predicting a\n",
      "patient's outcome is an essential part of such systems, for which the use of\n",
      "deep neural networks has shown promising results. However, the patterns learned\n",
      "by these networks are mostly opaque and previous work revealed flaws regarding\n",
      "the reproduction of unintended biases. We thus introduce an extendable testing\n",
      "framework that evaluates the behavior of clinical outcome models regarding\n",
      "changes of the input. The framework helps to understand learned patterns and\n",
      "their influence on model decisions. In this work, we apply it to analyse the\n",
      "change in behavior with regard to the patient characteristics gender, age and\n",
      "ethnicity. Our evaluation of three current clinical NLP models demonstrates the\n",
      "concrete effects of these characteristics on the models' decisions. They show\n",
      "that model behavior varies drastically even when fine-tuned on the same data\n",
      "and that allegedly best-performing models have not always learned the most\n",
      "medically plausible patterns.\n",
      "--------------------------------------------------------------------------------\n",
      "A Computational Model for Logical Analysis of Data\n",
      "Frédéric Saubion\n",
      "2022\n",
      "Initially introduced by Peter Hammer, Logical Analysis of Data is a\n",
      "methodology that aims at computing a logical justification for dividing a group\n",
      "of data in two groups of observations, usually called the positive and negative\n",
      "groups. Consider this partition into positive and negative groups as the\n",
      "description of a partially defined Boolean function; the data is then processed\n",
      "to identify a subset of attributes, whose values may be used to characterize\n",
      "the observations of the positive groups against those of the negative group.\n",
      "  LAD constitutes an interesting rule-based learning alternative to classic\n",
      "statistical learning techniques and has many practical applications.\n",
      "Nevertheless, the computation of group characterization may be costly,\n",
      "depending on the properties of the data instances. A major aim of our work is\n",
      "to provide effective tools for speeding up the computations, by computing some\n",
      "\\emph{a priori} probability that a given set of attributes does characterize\n",
      "the positive and negative groups. To this effect, we propose several models for\n",
      "representing the data set of observations, according to the information we have\n",
      "on it. These models, and the probabilities they allow us to compute, are also\n",
      "helpful for quickly assessing some properties of the real data at hand;\n",
      "furthermore they may help us to better analyze and understand the computational\n",
      "difficulties encountered by solving methods.\n",
      "  Once our models have been established, the mathematical tools for computing\n",
      "probabilities come from Analytic Combinatorics. They allow us to express the\n",
      "desired probabilities as ratios of generating functions coefficients, which\n",
      "then provide a quick computation of their numerical values. A further,\n",
      "long-range goal of this paper is to show that the methods of Analytic\n",
      "Combinatorics can help in analyzing the performance of various algorithms in\n",
      "LAD and related fields.\n",
      "--------------------------------------------------------------------------------\n",
      "Studying Generalization Through Data Averaging\n",
      "Carlos A. Gomez-Uribe\n",
      "2022\n",
      "The generalization of machine learning models has a complex dependence on the\n",
      "data, model and learning algorithm. We study train and test performance, as\n",
      "well as the generalization gap given by the mean of their difference over\n",
      "different data set samples to understand their ``typical\" behavior. We derive\n",
      "an expression for the gap as a function of the covariance between the model\n",
      "parameter distribution and the train loss, and another expression for the\n",
      "average test performance, showing test generalization only depends on\n",
      "data-averaged parameter distribution and the data-averaged loss. We show that\n",
      "for a large class of model parameter distributions a modified generalization\n",
      "gap is always non-negative. By specializing further to parameter distributions\n",
      "produced by stochastic gradient descent (SGD), along with a few approximations\n",
      "and modeling considerations, we are able to predict some aspects about how the\n",
      "generalization gap and model train and test performance vary as a function of\n",
      "SGD noise. We evaluate these predictions empirically on the Cifar10\n",
      "classification task based on a ResNet architecture.\n",
      "--------------------------------------------------------------------------------\n",
      "Model Zoos: A Dataset of Diverse Populations of Neural Network Models\n",
      "Damian Borth\n",
      "2022\n",
      "In the last years, neural networks (NN) have evolved from laboratory\n",
      "environments to the state-of-the-art for many real-world problems. It was shown\n",
      "that NN models (i.e., their weights and biases) evolve on unique trajectories\n",
      "in weight space during training. Following, a population of such neural network\n",
      "models (referred to as model zoo) would form structures in weight space. We\n",
      "think that the geometry, curvature and smoothness of these structures contain\n",
      "information about the state of training and can reveal latent properties of\n",
      "individual models. With such model zoos, one could investigate novel approaches\n",
      "for (i) model analysis, (ii) discover unknown learning dynamics, (iii) learn\n",
      "rich representations of such populations, or (iv) exploit the model zoos for\n",
      "generative modelling of NN weights and biases. Unfortunately, the lack of\n",
      "standardized model zoos and available benchmarks significantly increases the\n",
      "friction for further research about populations of NNs. With this work, we\n",
      "publish a novel dataset of model zoos containing systematically generated and\n",
      "diverse populations of NN models for further research. In total the proposed\n",
      "model zoo dataset is based on eight image datasets, consists of 27 model zoos\n",
      "trained with varying hyperparameter combinations and includes 50'360 unique NN\n",
      "models as well as their sparsified twins, resulting in over 3'844'360 collected\n",
      "model states. Additionally, to the model zoo data we provide an in-depth\n",
      "analysis of the zoos and provide benchmarks for multiple downstream tasks. The\n",
      "dataset can be found at www.modelzoos.cc.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(results['titles'])):\n",
    "    print(results['titles'][i])\n",
    "    print(results['authors'][i])\n",
    "    print(results['years'][i])\n",
    "    print(results['summary'][i])\n",
    "    print('----------------------------------------' * 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4524e0aa81b3ae16e25dcf9b33e91112c133dfc9ef43cc517d5da5b8fe0f3eee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
